{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e3959-de94-4dd4-9091-97ee75ffdd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_timestamp,\n",
    "    when,\n",
    "    sum as spark_sum,\n",
    "    lag,\n",
    "    unix_timestamp,\n",
    "    col,\n",
    "    monotonically_increasing_id,\n",
    "    udf\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import logging\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    PointStruct\n",
    ")\n",
    "\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5105a2-de4b-4cbc-9b43-4276f9c9918a",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession.builder.appName(\"test qdrant \").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"8g\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bc5c3-d416-4864-84b7-0d629de1e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV into a DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/test.csv\")\n",
    "\n",
    "# Display the first 2 rows\n",
    "# Show schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616c533-a911-4dc9-8369-f1859519be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Repartition for parallelism\n",
    "df = df.repartition(200)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. Convert `event_time` to Timestamp\n",
    "# ------------------------------------------------------------------------\n",
    "df = df.withColumn(\n",
    "    \"event_time\",\n",
    "    to_timestamp(\"event_time\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. Generate flags for event types\n",
    "# ------------------------------------------------------------------------\n",
    "df = (\n",
    "    df.withColumn(\"view_count\", when(col(\"event_type\") == \"view\", 1).otherwise(0))\n",
    "      .withColumn(\"cart_count\", when(col(\"event_type\") == \"cart\", 1).otherwise(0))\n",
    "      .withColumn(\"purchase_count\", when(col(\"event_type\") == \"purchase\", 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. Compute session-level totals\n",
    "# ------------------------------------------------------------------------\n",
    "df_totals = df.groupBy(\"user_session\").agg(\n",
    "    spark_sum(\"view_count\").alias(\"total_views\"),\n",
    "    spark_sum(\"cart_count\").alias(\"total_carts\"),\n",
    "    spark_sum(\"purchase_count\").alias(\"total_purchases\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7. Compute product-level features (views, carts, purchases) and join\n",
    "# ------------------------------------------------------------------------\n",
    "df_product = df.groupBy(\"user_session\", \"product_id\", \"user_id\").agg(\n",
    "    spark_sum(\"view_count\").alias(\"product_views\"),\n",
    "    spark_sum(\"cart_count\").alias(\"product_carts\"),\n",
    "    spark_sum(\"purchase_count\").alias(\"product_purchases\")\n",
    ")\n",
    "\n",
    "df_features = df_product.join(df_totals, on=\"user_session\", how=\"left\")\n",
    "\n",
    "df_features = (\n",
    "    df_features\n",
    "    .withColumn(\n",
    "        \"F1\",\n",
    "        when(col(\"total_views\") != 0, col(\"product_views\") / col(\"total_views\")).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"F2\",\n",
    "        when(col(\"total_carts\") != 0, col(\"product_carts\") / col(\"total_carts\")).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"F3\",\n",
    "        when(col(\"total_purchases\") != 0, col(\"product_purchases\") / col(\"total_purchases\")).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8. Create window for time-based features\n",
    "# ------------------------------------------------------------------------\n",
    "window_order = Window.partitionBy(\"user_session\").orderBy(\"event_time\")\n",
    "\n",
    "df_time = (\n",
    "    df.withColumn(\"prev_event_time\", lag(\"event_time\").over(window_order))\n",
    "      .withColumn(\n",
    "          \"time_spent_seconds\",\n",
    "          unix_timestamp(\"event_time\") - unix_timestamp(\"prev_event_time\")\n",
    "      )\n",
    "      .na.fill(0, subset=[\"time_spent_seconds\"])\n",
    "      .withColumn(\"time_spent\", col(\"time_spent_seconds\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 9. Aggregate time spent per product vs. total\n",
    "# ------------------------------------------------------------------------\n",
    "df_time_agg = df_time.groupBy(\"user_session\", \"product_id\", \"user_id\").agg(\n",
    "    spark_sum(\"time_spent\").alias(\"product_time_spent\")\n",
    ")\n",
    "\n",
    "df_total_time = df_time.groupBy(\"user_session\").agg(\n",
    "    spark_sum(\"time_spent\").alias(\"total_time_spent\")\n",
    ")\n",
    "\n",
    "df_features = (\n",
    "    df_features\n",
    "    .join(df_time_agg, on=[\"user_session\", \"product_id\", \"user_id\"], how=\"left\")\n",
    "    .join(df_total_time, on=\"user_session\", how=\"left\")\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"F4\",\n",
    "    when(col(\"total_time_spent\") != 0, col(\"product_time_spent\") / col(\"total_time_spent\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 10. Define weights and compute a final score\n",
    "# ------------------------------------------------------------------------\n",
    "w1, w2, w3, w4 = 0.1, 0.25, 0.45, 0.2\n",
    "df_features = (\n",
    "    df_features\n",
    "    .withColumn(\n",
    "        \"score\",\n",
    "        w1 * col(\"F1\") + w2 * col(\"F2\") + w3 * col(\"F3\") + w4 * col(\"F4\")\n",
    "    )\n",
    "    .fillna({\"score\": 0})\n",
    ")\n",
    "\n",
    "final_df = df_features.select(\"user_id\", \"product_id\", \"score\")\n",
    "logger.info(\"Feature engineering completed.\")\n",
    "final_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5896b-0c75-4680-8b5b-3c7065b71940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98331a33-f777-4659-b6b4-52c0f3c234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "\n",
    "\n",
    "# Chuyển đổi user_id thành chuỗi và đóng gói trong một danh sách (Word2Vec yêu cầu input là danh sách từ)\n",
    "df_string = df.withColumn(\"user_id_str\", col(\"user_id\").cast(\"string\"))\n",
    "df_words = df_string.withColumn(\"user_id_list\", split(col(\"user_id_str\"), \"\"))  # Mỗi ký tự là một \"word\"\n",
    "\n",
    "# Áp dụng Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"user_id_list\", outputCol=\"user_embedding\")\n",
    "model = word2Vec.fit(df_words)\n",
    "df_with_embedding = model.transform(df_words)\n",
    "\n",
    "# Chọn các cột cần thiết\n",
    "df_final = df_with_embedding.select(\"user_id\", \"product_id\", \"score\", \"user_embedding\")\n",
    "\n",
    "# Hiển thị DataFrame với embedding\n",
    "df_final.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628a89b-7dad-4174-8314-fcac9ec8a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "df_pandas = df_final.select(\"user_id\", \"product_id\", \"score\", \"user_embedding\").toPandas()\n",
    "collection_name = \"user_embeddings\"\n",
    "\n",
    "# Kiểm tra xem collection đã tồn tại chưa\n",
    "collections = client.get_collections()\n",
    "if collection_name not in [collection.name for collection in collections.collections]:\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=10, distance=Distance.COSINE)  # Thay `size` theo kích thước embedding của bạn\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' đã được tạo.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' đã tồn tại.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a2559-0daf-4c33-a3fe-d8ed8bcd9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, udf\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# Giả sử bạn đã có biến `collection_name` định nghĩa tên collection trong Qdrant\n",
    "\n",
    "# Bước 1: Xử lý dữ liệu trực tiếp trong Spark\n",
    "# Giả sử 'user_embedding' là cột Vector, chuyển nó thành list\n",
    "def vector_to_list(vector):\n",
    "    return vector.toArray().tolist() if hasattr(vector, 'toArray') else vector\n",
    "\n",
    "vector_to_list_udf = udf(vector_to_list, ArrayType(FloatType()))\n",
    "\n",
    "df_preprocessed = df_final.select(\n",
    "    regexp_replace(col(\"user_id\"), \",\", \"\").cast(IntegerType()).alias(\"user_id\"),\n",
    "    \"product_id\",\n",
    "    \"score\",\n",
    "    vector_to_list_udf(col(\"user_embedding\")).alias(\"user_embedding\")\n",
    ")\n",
    "\n",
    "# Bước 2: Định nghĩa hàm upsert cho mỗi phân vùng\n",
    "def upsert_partition(partition):\n",
    "    client = QdrantClient()  # Khởi tạo client Qdrant trong mỗi phân vùng\n",
    "    points = []\n",
    "    for row in partition:\n",
    "        point = PointStruct(\n",
    "            id=row['user_id'],  # Sử dụng user_id làm ID point\n",
    "            vector=row['user_embedding'],\n",
    "            payload={\n",
    "                \"product_id\": row['product_id'],\n",
    "                \"score\": row['score']\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "        \n",
    "        # Upsert mỗi 1000 points để tăng hiệu suất\n",
    "        if len(points) >= 1000:\n",
    "            client.upsert(collection_name=collection_name, points=points)\n",
    "            print(f\"Đã upsert 1000 points\")\n",
    "            points = []\n",
    "    \n",
    "    # Upsert các points còn lại\n",
    "    if points:\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "        print(f\"Đã upsert {len(points)} points cuối cùng\")\n",
    "\n",
    "# Bước 3: Áp dụng upsert cho từng phân vùng\n",
    "df_preprocessed.rdd.foreachPartition(upsert_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4565f-497c-4bce-ac18-cb080b7c00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4e08e-1d04-4802-aaf9-ee283f2e68cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
